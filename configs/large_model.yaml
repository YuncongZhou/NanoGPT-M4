# Large model configuration - max for 24GB RAM
model:
  n_layer: 12
  n_head: 12
  n_embd: 768
  dropout: 0.2
  block_size: 1024
  bias: false

training:
  batch_size: 4
  learning_rate: 1e-4
  max_iters: 20000
  warmup_iters: 500
  eval_interval: 1000
  eval_iters: 50
  gradient_accumulation_steps: 16
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.99
  grad_clip: 1.0
  decay_lr: true
  lr_decay_iters: 20000
  min_lr: 1e-5

device:
  type: mps
  mixed_precision: false
  compile: false

data:
  dataset: shakespeare
  tokenizer_type: character

logging:
  log_interval: 50
  save_interval: 2000

output:
  out_dir: checkpoints/large_model
  always_save_checkpoint: false

seed: 1337