# Medium model configuration - for serious training
model:
  n_layer: 8
  n_head: 8
  n_embd: 512
  dropout: 0.2
  block_size: 512
  bias: false

training:
  batch_size: 8
  learning_rate: 2e-4
  max_iters: 10000
  warmup_iters: 200
  eval_interval: 500
  eval_iters: 50
  gradient_accumulation_steps: 8
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  decay_lr: true
  lr_decay_iters: 10000
  min_lr: 2e-5

device:
  type: mps
  mixed_precision: false
  compile: false

data:
  dataset: shakespeare
  tokenizer_type: character

logging:
  log_interval: 20
  save_interval: 1000

output:
  out_dir: checkpoints/medium_model
  always_save_checkpoint: false

seed: 1337